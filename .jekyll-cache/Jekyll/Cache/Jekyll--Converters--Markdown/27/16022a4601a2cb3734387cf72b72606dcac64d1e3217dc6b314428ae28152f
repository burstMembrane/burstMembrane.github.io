I"@B<h1 id="chapter-four">CHAPTER FOUR</h1>
<h2 id="user-interface-design-algorithmic-mediation-in-practice-and-process">User Interface Design: Algorithmic Mediation in Practice and Process</h2>

<p>This project is focused on the creative and critical possibilities of experimental artistic applications of Computer Vision algorithms. Computer Vision is an area of research where image data is processed by computers attempting to synthesise qualities of human vision. It is primarily concerned with the classification, quantification and tracking of objects within image data. One method to achieve this is the use of a contrast-based detection algorithm.</p>

<p><img src="images/thesis/image022.jpg" alt="fig 12" /></p>
<h3 id="fig-12-bounding-boxes-drawn-on-raw-image-sep-2019">Fig 12: Bounding boxes drawn on raw image, Sep 2019</h3>

<p>To a computer, an image is perceived as a constantly shifting stream of binary data. An image is captured through the lens of a camera to a sensor and stored as a file. This file is retrieved and streamed through a codec to software that loops over this data and interprets it on a screen. When received by most software, that colour is represented as a separate value from zero to 255 in an array. In an 8-bit RGB colourspace, an all red pixel would appear to be (255, 0, 0), a green, (0, 255, 0) and a blue (0, 0, 255). These pixel streams are read in a loop each time a frame is read from the file; and each pixel is stored in an array which can be retrieved, manipulated and read. In the OpenCV algorithms, RGB images are converted to greyscale, where each pixel is represented as a single luminance value between zero and 255.</p>

<p><img src="images/thesis/image024.jpg" alt="fig 13" /></p>
<h3 id="fig-13-bounding-boxes-drawn-on-binary-image-sep-2019">Fig 13: Bounding boxes drawn on binary image, Sep 2019</h3>

<p>Further thresholding creates a binary image, which limits these values in search of areas of brightness in the frame. If a detected contrast of luminance values passes a certain threshold, then it can be ascertained that there is a subject and background, or a region of interest (ROI) within the frame. This ROI can then be cropped and saved for later use or identified by a bounding box surrounding it. I began by finding these regions of interest in image data with static thresholding. Next I converted the image to a series of pixel values between 0-255 and, searching through each pixel, I would target similarities and contrast ratios within an image that pass a certain threshold. The algorithm then extracts the ROIs within the frame and sorts them from largest to smallest.</p>

<p>To a computer, an image is perceived as a constantly shifting stream of binary data.  Changes in the region of interest occurred on a frame-by-frame scale, leading to output videos that would frenetically switch content many times a second: a timescale that was not human-readable. This, however, had the effect of generating a feeling of nausea in the viewer, as the image changed too quickly to identify specific details within the frame. The level of abstraction created by this raw thresholding algorithm, while of some interest, undermined the legibility of the content of the video being processed to such an extent that the content became redundant.</p>

<p><img src="images/thesis/image026.jpg" alt="fig 14" /></p>
<h3 id="fig-14-a-low-resolution-region-of-interest-roi-drawn-from-a-video-feed">Fig 14: A low resolution Region of Interest (ROI) drawn from a video feed.</h3>
<p>This was the extreme end of computer vision — the images were being cropped so fast that the computer could process them, but I could not. To continue my experiments I understood that I needed to slow things down. I needed to give the computer a steady gaze, one that picked details selectively from within the frame and held them long enough for an audience to gain some insight into their meaning. This would allow the spectator to get a better idea of the details being picked out by the algorithm as a point of access, and the necessary frame of reference that this was in fact video content, not generated visuals. I experimented with a procedure called ‘aggregation’ to achieve this. Aggregation takes a number of video or live input frames and blends them together over time, showing the patterns of movement present within a video. From extensive experimentation I understood that I could slow the timescale of the thresholding mechanism to something more humanly readable by aggregating frames before processing the output. This aggregation is controlled by an alpha value, which regulates the weighting of frames over time. For example, an alpha value of 1 would show the frames in real time, whilst an alpha value of 0.01 would show those frames averaged in real-time divided by ten. For a 24fps video with an alpha value of 0.01, 24 frames would be accumulated in order to output one second of content. I found that by averaging movement in the data before thresholding and cropping, I could slow down the rate of change that occurred in processing to bring it closer to a format that’s humanly readable.</p>

<p><img src="images/thesis/image028.jpg" alt="fig 15" /></p>
<h3 id="fig-15-aggregation-study-aug-2019">Fig 15: Aggregation Study, Aug 2019</h3>

<p>From there I proceeded to experimentation with recomposing the output image onto a grid. The code would find the regions of interest in an image, store them in an array and then output them on a grid with a predefined size. The size of each ‘cell’ is a division of the width of the output image, so for a 1920 x 1080 input resolution to be divided into 2 cells, a cell size of 960 would be required. Further subdivisions of cell size created a grid which would show more details within the frame. The methods of thresholding, cropping, aggregation and recomposition formed the main algorithmic manipulations enacted on the image data. The sound  component of the feed was processed in a similar way. Areas of silence within a file would be sought by the algorithm, skipping around in a buffer to an area of predefined loudness.</p>

<p><img src="images/thesis/image029.jpg" alt="fig 16" /></p>
<h3 id="fig-16-rois-recomposed-on-a-grid-aug-2019">Fig 16: ROIs recomposed on a grid, Aug 2019</h3>

<p>As discussed in previous chapters, the main methods of surveillance used by major corporations upon against the individual today take the form of data extraction and manipulation through computing technologies (Zuboff 2015, p.79).  Once I understood the literal and symbolic links my work on thresholding and aggregation algorithms had with such data extraction and manipulation, I attended more closely to methods by which the spectator might productively and critically interface with the video work itself. David Rokeby in The Construction of Experience: Interface as Content (1998), discusses the relationship between audience, media and interface in art (Rokeby 1998, p.1). For Rokeby, the direct visual experience of images on a display medium, such as a TV screen, remains a thing that “happen to you” (1998, p.1). To avoid such a passive spectatorial relationship in my work I experimented with methods of constructing an display interface that might, in addition to ‘showing the content’, also evoke questions about how that interface functions. Specifically, I wanted to explore how video data is transmitted, manipulated and stored. The computer terminal, specifically the desktop computer, is today still one of the most representative objects through which the spectator interacts with surveillance algorithms directly. It was my hope that the use these interfaces would provide a contextual basis for further experimentation. 
Towards highlighting the connectedness of global surveillance, I used multiple networked computers, with each terminal sharing the same “raw” dataset and processing it in a progressively fragmented way. This created a server-client structure to the network and enforced a hierarchy between the most humanly readable and the least.</p>

<p><img src="images/thesis/image030.png" alt="fig 15" /></p>
<h3 id="fig-17-mockup-diagram-mediation-1-2019--showing-linear-screen-structure">Fig 17: Mockup Diagram, Mediation #1 (2019) — showing linear screen structure</h3>

<p>Rokeby writes about the considerable gap in potential between human perceptual interfaces, sense and memory, and that of computing technologies (1998, p.4). The relationship between the spectator and the work will always involve a perceptual constriction in order to fit the human’s presence and actions to the interface (1998, p.4). This sentiment echoes my initial struggles with human readability as a result of algorithmic mediation. In light of this understanding, it was important that the user’s experience be carefully adjusted to encourage the human perceptual system to respond actively, rather than assuming that the content, or the interface, will synthesise associations on its own. To contend with the incredibly complex and varied experience of human perception, the designer of an interface must ensure that it only does a few things, but does them very well. This underscored what was at issue in choices made regarding the work’s interface. 
For example, to enhance the perceptual impact of the work I wanted to create associations with an everyday interface, such as a computer or smartphone. For Rokeby, a key aspect to the creation of interfaces is time; The longer a person spends using an interface, the stronger their association with it (Rokeby 1998, p.2). Therefore, an interface such as a YouTube video played back on a desktop computer, utilised daily by millions around the world for entertainment or information, comes pre-loaded with its own perceptual imprint. This imprint can be explored and critiqued in an artistic context, perhaps drawing on the users’ subconscious associations and frustrations.</p>

<p><img src="images/thesis/image032.jpg" alt="fig 18" /></p>
<h3 id="fig-18-first-trial-presentation-may-2019---evenly-spaced-screens">Fig 18: First trial presentation May 2019 - evenly spaced screens</h3>
<p>By using more than one computer I am able to show the progressive breakdown and restructuring of the data in a way that could be explored physically by an audience in a gallery setting. During the first trial presentation I focused on highlighting the networked nature of the image, as well as negating the ‘loaded’ interface of the computer terminal. By loaded I mean that implicit in our understanding of the computer interface is an expectation of interactivity and control. It is via the keyboard and the mouse that we normally interface with computers and, as such, these are the interfaces through which digital mediation into life is enacted (Zuboff 2015, p.77). I continued this exploration in later trial presentations. By removing these interface devices from the computers and showing them facing one another, I aimed to show that there was communication happening between the computers that the user had no control over — they could see, but not touch.</p>

<p>In Semester Two trials, I used three screens and a longer video presentation which spoofed the vapid, consumerist, and alienating aspects of digital culture. Hito Steyerl’s concepts surrounding the digital image were an inspiration in my exploration of these themes. To Steyerl, Circulationism is not concerned with making an image but of “the public relations of images across social networks … advertisement and alienation, and about being as suavely vacuous as possible” (Steyerl 2013, p.7). These videos used digital cinematographic tropes in order to mimic the form of videos found online. The video project that fed the thresholding algorithm is a fifteen minute stream consisting of autobiographical vignettes on technological mediation into human life: readings from Society of the Spectacle, a slow, Kubrick-style zoom watching Netflix, gonzo over-the-shoulder shots of myself using a smartphone, a fake product review-video, and a gestural performance video. Together, these seemingly disparate video pieces, which reflected something of my own experience and frustration with big data and consumer culture, formed a stream of content that could be dipped into and out of by the audience. The use of a “stream” was an exploration of what I see as the usual transience of encounter with artworks (especially in a trial presentation setting). The ideal duration of encounter for this work need not be more than one to two minutes, with the whole video presentation not intended to be watched from start to finish.</p>

<p><img src="images/thesis/image033.png" alt="fig 19a" />
<img src="images/thesis/image035.png" alt="fig 19b" /></p>
<h3 id="fig-19a-b-video-stills---networked-mediation-2019">Fig 19a-b: Video Stills  — Networked Mediation (2019)</h3>
<p> 
Though the audience encounters the video stream perhaps only briefly, my aim with both the image-data manipulation and the arrangement of the screen interface, is to trigger subtle subconscious associations with the themes and ideas behind the piece. To achieve this I placed the screen showing “raw” video uncomfortably close to the wall and on the floor, hoping that would restrict access to a longer viewing. However, it became apparent during critiques that the audience was not sure if they were meant to view the video presentation in full, or dip in and out of it like a stream. The progressive breakdown and recomposition of the image across the screens was aimed towards the creation of different levels of engagement with the content. In the critique sessions people commented on feeling “frustrated” with their inability to easily understand and process the more mediated content. Further, by placing the “raw” content screen close to the wall, I attempted to encourage an intimate but brief engagement with the “raw” video footage. The effect was to encourage audiences to view the three screens as a display system and to find the relationships between the images, rather than foregrounding the more human-readable and digestible content.</p>

<p>For Clemens &amp; Nash, the appearance of media in the world is due to a “protocol driven modulation process to and from display states” (2015, p.11). Display is thus the individuation of data from its undifferentiated state to a simulated phenomenon in the real world (2015, p.13). In its raw state, that of the physical process of switching between electrical states, data is undifferentiated. It needs to be translated into a display register or medium in order to be understandable by a human at any level. There are however different technical levels at which the data is viewed, and these involve the convergence of widely different protocols and algorithmic mediators before the data is readable by a lay audience. For example, a YouTube video would be viewed very differently by an everyday user, software designer, or computer hardware engineer, as the levels of differentiation and modulation of data would be specific to their understanding of the system. My intention with this project is to introduce to an audience not formally trained in these levels of data transmission and manipulation an idea of what modulation may be happening behind the scenes. Towards that end, in my work I have found it useful to think of the display as the specific modulation or differentiation of data (Clemens and Nash 2015, p.10). The construction of a digital image (or sound) is in this reading a performance of data. What appears on the screen is simply quantified to predictable results by the modulating device (e.g. using codecs and RGB display protocols) (2015, p.12). This process of modulation of data between different algorithmically-mediated states is the primary process that connects the seemingly disparate elements displayed on screen.</p>

<p>My practice-led-research thus investigates the modulation of data, code, spectator, technology, surveillance, and content between physical and symbolic states of display. The spectator interacts with work only insomuch as they are able to comprehend the localization of data to a display medium. Through extensive experimentation with image-data, content and space I have aimed to construct a detournement of Spectacular digital culture. The human’s place in an automated world remains unclear, but the ongoing goal of my research and practice is to shine light on concerns surrounding computing technolgoies and their ongoing effects on how we see ourselves and communicate with others.</p>

<p><img src="images/thesis/image038.jpg" alt="fig 20a" />
<img src="images/thesis/image037.jpg" alt="fig 20b" /></p>
<h3 id="-fig-20a-b-second-trial-presentation-sep-2019-installation-views">### Fig 20a-b: Second Trial Presentation, Sep 2019, Installation Views</h3>

:ET